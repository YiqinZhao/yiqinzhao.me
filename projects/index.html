<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"/><meta charSet="utf-8"/><title>Projects | Yiqin Zhao</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/default.min.css"/><script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/highlight.min.js"></script><link rel="stylesheet" href="/css/base.css"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="default"/><meta name="apple-mobile-web-app-title" content="YiqinZhao"/><link href="/splashscreens/iphone5_splash.png" media="(device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link href="/splashscreens/iphone6_splash.png" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link href="/splashscreens/iphoneplus_splash.png" media="(device-width: 621px) and (device-height: 1104px) and (-webkit-device-pixel-ratio: 3)" rel="apple-touch-startup-image"/><link href="/splashscreens/iphonex_splash.png" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)" rel="apple-touch-startup-image"/><link href="/splashscreens/iphonexr_splash.png" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link href="/splashscreens/iphonexsmax_splash.png" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)" rel="apple-touch-startup-image"/><link href="/splashscreens/ipad_splash.png" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link href="/splashscreens/ipadpro1_splash.png" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link href="/splashscreens/ipadpro3_splash.png" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link href="/splashscreens/ipadpro2_splash.png" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)" rel="apple-touch-startup-image"/><link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#ffffff"/><link rel="shortcut icon" href="/icons/favicon.ico" type="image/x-icon"/><meta name="msapplication-TileColor" content="#ffffff"/><meta name="theme-color" content="#ffffff"/><script type="text/javascript" src="/js/base.js"></script><meta name="next-head-count" content="28"/><link rel="preload" href="/_next/static/cwqh0joOOSi8uqnAsgZ0M/pages/%5Bname%5D.js" as="script"/><link rel="preload" href="/_next/static/cwqh0joOOSi8uqnAsgZ0M/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-035ac2b14bde147cb4a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.674a7af05ea3854fc387.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-69a761dabfa718f9e8df.js" as="script"/></head><body><div id="__next"><div class="main-container"><div class="fb2nrvw "><ul class="f1ajzahn"><a href="/" target=""><li class="fcai4ky ">home</li></a><a href="/news" target=""><li class="fcai4ky ">news</li></a><a href="/projects" target=""><li class="fcai4ky f2igm3j">projects</li></a><a href="/publication" target=""><li class="fcai4ky ">publication</li></a><a href="/articles" target=""><li class="fcai4ky ">articles</li></a><a href="/cv.pdf" target="_blank"><li class="fcai4ky ">cv</li></a></ul><div class="fkhz08q f1yhh1tt" style="justify-content:flex-start"><span>Find me on internet:</span><div style="justify-content:flex-start" class="fq2lkuj "><a href="https://weibo.com/Spartan093" target="_blank" class="f1e23ih4"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="weibo" class="svg-inline--fa fa-weibo fa-w-16 f5ddpva" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"></path></svg></a><a href="https://github.com/YiqinZhao" target="_blank" class="f1e23ih4"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16 f5ddpva" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></div><header id="header" class="f1vke0um "><a href="/" class="fpdbkr8"><img class="f2ehq0x" src="/images/qin.svg" alt=""/></a><nav class="fzvi4lv"><ul class="f86z0sq"><a href="/" target=""><li class="f15bnvkv ">home</li></a><a href="/news" target=""><li class="f15bnvkv ">news</li></a><a href="/projects" target=""><li class="f15bnvkv f2igm3j">projects</li></a><a href="/publication" target=""><li class="f15bnvkv ">publication</li></a><a href="/articles" target=""><li class="f15bnvkv ">articles</li></a><a href="/cv.pdf" target="_blank"><li class="f15bnvkv ">cv</li></a></ul></nav><div class="f11x3dlo"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars fa-w-14 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></div></header><div class="f1py53zp  "><div class="f8xi195 "></div><div class=" markdown"><h1 id="introduction">Introduction</h1>
<p>Currently I'm a research student in Worcester Polytechnic Institute <a href="https://cake-lab.github.io">CakeLab</a>, my research interests include the broad area of computer system and machine learning. You can see the <a href="#System-and-Machine-Learning">projects</a> I'm working on now. Previously in my undergraduate study, I'm a student in the cognition and affective computing lab, Tianjin Normal University. In there, my research <a href="#-Affective-Computing">project</a> is focused on speech emotion recognition with deep learning.</p>
<p>Besides my research, I'm also a geek and a developer. I love to explore and play with popular programming languages and frameworks. Specifically, I love JavaScript and Swift. And I also do creative artworks. <a href="##Hackathon-Projects">Here</a> are serval hackathon and open source projects I created.</p>
<h2 id="system-and-machine-learning">System and Machine Learning</h2>
<p>Still Working on it...</p>
<h2 id="affective-computing">Affective Computing</h2>
<h3 id="deep-spectrum-feature-representations-for-speech-emotion-recognition"><a href="/projects/deep-spectrum">Deep Spectrum Feature Representations for Speech Emotion Recognition</a></h3>
<p>Automatically detecting emotional state in human speech, which plays an effective role in areas of human machine interactions, has been a difficult task for machine learning algorithms. Previous work for emotion recognition have mostly focused on the extraction of carefully hand-crafted and tailored features. Recently, spectrogram representations of emotion speech have achieved competitive per- formance for automatic speech emotion recognition. In this work we propose a method to tackle the problem of deep features, herein denoted as deep spectrum features, extraction from the spectrogram by leveraging Attention-based Bidirectional Long Short-Term Mem- ory Recurrent Neural Networks with fully convolutional networks. The learned deep spectrum features are then fed into a deep neural network (DNN) to predict the final emotion. The proposed model is then evaluated on the Interactive Emotional Dyadic Motion Cap- ture (IEMOCAP) dataset to validate its effectiveness. Promising results indicate that our deep spectrum representations extracted from the proposed model perform the best, 65.2% for weighted accuracy and 68.0% for unweighted accuracy when compared to other existing methods. We then compare the performance of our deep spectrum features with two standard acoustic feature repre- sentations for speech-based emotion recognition. When combined with a support vector classifier, the performance of the deep feature representations extracted are comparable with the conventional features. Moreover, we also investigate the impact of different fre- quency resolutions of the input spectrogram on the performance of the system.</p>
<h2 id="hackathon-projects">Hackathon Projects</h2>
<h3 id="my-personal-blog-yiqinzhao.me"><a href="/projects/blog">My Personal Blog yiqinzhao.me</a></h3>
<p><strong>TODO</strong></p>
</div><div class="f8xi195 "></div></div><div class="f8xi195 "></div><footer class="fhui6ea">Â© Yiqin Zhao <!-- -->2019<!-- -->. Last Updated: <!-- -->Oct 28, 2019</footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"content":{"meta":null,"abstract":null,"body":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eCurrently I'm a research student in Worcester Polytechnic Institute \u003ca href=\"https://cake-lab.github.io\"\u003eCakeLab\u003c/a\u003e, my research interests include the broad area of computer system and machine learning. You can see the \u003ca href=\"#System-and-Machine-Learning\"\u003eprojects\u003c/a\u003e I'm working on now. Previously in my undergraduate study, I'm a student in the cognition and affective computing lab, Tianjin Normal University. In there, my research \u003ca href=\"#-Affective-Computing\"\u003eproject\u003c/a\u003e is focused on speech emotion recognition with deep learning.\u003c/p\u003e\n\u003cp\u003eBesides my research, I'm also a geek and a developer. I love to explore and play with popular programming languages and frameworks. Specifically, I love JavaScript and Swift. And I also do creative artworks. \u003ca href=\"##Hackathon-Projects\"\u003eHere\u003c/a\u003e are serval hackathon and open source projects I created.\u003c/p\u003e\n\u003ch2 id=\"system-and-machine-learning\"\u003eSystem and Machine Learning\u003c/h2\u003e\n\u003cp\u003eStill Working on it...\u003c/p\u003e\n\u003ch2 id=\"affective-computing\"\u003eAffective Computing\u003c/h2\u003e\n\u003ch3 id=\"deep-spectrum-feature-representations-for-speech-emotion-recognition\"\u003e\u003ca href=\"/projects/deep-spectrum\"\u003eDeep Spectrum Feature Representations for Speech Emotion Recognition\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAutomatically detecting emotional state in human speech, which plays an effective role in areas of human machine interactions, has been a difficult task for machine learning algorithms. Previous work for emotion recognition have mostly focused on the extraction of carefully hand-crafted and tailored features. Recently, spectrogram representations of emotion speech have achieved competitive per- formance for automatic speech emotion recognition. In this work we propose a method to tackle the problem of deep features, herein denoted as deep spectrum features, extraction from the spectrogram by leveraging Attention-based Bidirectional Long Short-Term Mem- ory Recurrent Neural Networks with fully convolutional networks. The learned deep spectrum features are then fed into a deep neural network (DNN) to predict the final emotion. The proposed model is then evaluated on the Interactive Emotional Dyadic Motion Cap- ture (IEMOCAP) dataset to validate its effectiveness. Promising results indicate that our deep spectrum representations extracted from the proposed model perform the best, 65.2% for weighted accuracy and 68.0% for unweighted accuracy when compared to other existing methods. We then compare the performance of our deep spectrum features with two standard acoustic feature repre- sentations for speech-based emotion recognition. When combined with a support vector classifier, the performance of the deep feature representations extracted are comparable with the conventional features. Moreover, we also investigate the impact of different fre- quency resolutions of the input spectrogram on the performance of the system.\u003c/p\u003e\n\u003ch2 id=\"hackathon-projects\"\u003eHackathon Projects\u003c/h2\u003e\n\u003ch3 id=\"my-personal-blog-yiqinzhao.me\"\u003e\u003ca href=\"/projects/blog\"\u003eMy Personal Blog yiqinzhao.me\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTODO\u003c/strong\u003e\u003c/p\u003e\n"},"name":"projects","date":"Oct 28, 2019"}},"page":"/[name]","query":{"name":"projects"},"buildId":"cwqh0joOOSi8uqnAsgZ0M","nextExport":true}</script><script async="" data-next-page="/[name]" src="/_next/static/cwqh0joOOSi8uqnAsgZ0M/pages/%5Bname%5D.js"></script><script async="" data-next-page="/_app" src="/_next/static/cwqh0joOOSi8uqnAsgZ0M/pages/_app.js"></script><script src="/_next/static/runtime/webpack-035ac2b14bde147cb4a8.js" async=""></script><script src="/_next/static/chunks/commons.674a7af05ea3854fc387.js" async=""></script><script src="/_next/static/runtime/main-69a761dabfa718f9e8df.js" async=""></script></body></html>