<html><head><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta charset="utf-8"><link rel="stylesheet" href="/assets/css/base.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css"><link rel="stylesheet" media="(prefers-color-scheme:light)" href="https://cdn.jsdelivr.net/npm/highlightjs@9.16.2/styles/tomorrow.css"><link rel="stylesheet" media="(prefers-color-scheme:dark)" href="https://cdn.jsdelivr.net/npm/highlightjs@9.16.2/styles/atom-one-dark.css"><script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/highlight.min.js"></script><title>All about Python Performance Optimization | Yiqin Zhao</title><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="apple-mobile-web-app-title" content="YiqinZhao"><link rel="apple-touch-icon" sizes="180x180" href="/assets/site-icons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/site-icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/site-icons/favicon-16x16.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="shortcut icon" href="/assets/site-icons/favicon.ico" type="image/x-icon"><meta name="msapplication-TileColor" content="#ffffff"><meta name="theme-color" content="#ffffff"><style>body{margin:0}img{width:100%}a{color:var(--font-color)}</style><style>.title{font-size:3rem;font-weight:700;margin:0 0 10px 0}.meta-row{border-bottom:1px dashed rgba(0,0,0,.4);margin-bottom:20px;opacity:.7;font-style:italic;flex-wrap:wrap;padding:10px 0}.meta-row>*{line-height:1.5rem}.date{margin:0 1rem}.tag{margin-right:.5rem}.markdown strong>img{border:1px solid rgba(0,0,0,.6)}.hero-wrapper{background-color:#f4f5f7;padding:0 0 40px 0}.hero-container{padding:20px 0;position:relative;align-items:flex-start}.markdown strong>img{border:1px solid rgba(0,0,0,.6)}@media (max-width:480px){.meta-row{flex-wrap:wrap}}@media (prefers-color-scheme:dark){.label-icon{filter:invert(1)}.markdown strong>img{filter:invert(.85)}}</style><style>.nav-container{width:100%;height:60px;padding:20px 0;display:flex;flex-direction:row;align-items:center;justify-content:space-between}.logo{font-size:1.5rem;font-weight:700;opacity:.7;cursor:pointer;color:var(--font-color);transition:opacity .1s linear}.logo:hover{opacity:1}.menu-item{font-size:1.3rem;font-weight:700;opacity:.7;text-transform:capitalize;margin:0 17px;padding:0 5px;cursor:pointer;transition:opacity .1s linear,border-bottom .1s linear}.menu-item:hover{opacity:1;border-bottom:3px solid #979797}.menu-item-active{opacity:1;border-bottom:3px solid #979797}.menu-item:nth-last-child(1){margin-right:0}.menu-row>a{color:var(--font-color)}.menu-row>a:hover{text-decoration:none}.menu-button{display:none}.mobile-nav{width:100%;display:none}@media (prefers-color-scheme:dark){#menu-button-icon{filter:invert()}}@media (min-width:320px) and (max-width:768px){body{padding-top:61px}.hero-header{overflow:hidden}.logo{display:none}.nav-container{padding:0 0;background-color:rgba(57,57,57,.9);position:fixed;top:0;border-bottom:1px solid #636363;backdrop-filter:blur(10px);-webkit-backdrop-filter:blur(10px);z-index:1;justify-content:flex-end}.menu-item-active{border-bottom:none}.menu-button{display:block;margin-right:20px}#menu-button-icon{height:17px;width:24px}.mobile-nav{height:60px;display:flex}.mobile-nav>.nav-title{font-size:1.3rem;font-weight:700;text-transform:capitalize;margin:0 17px;padding:0 5px}.nav-container{height:auto;flex-direction:column}.menu-row{padding:5px 0;flex-wrap:wrap;overflow:hidden;justify-content:space-between}.hide-menu-row{height:0;padding:0}.menu-row>.menu-item{margin:10px 17px}}</style><style>.foot{opacity:.6;border-top:1px solid var(--line-color);margin-top:20px;padding:20px 0;font-size:.8rem}.foot a{color:var(--font-color);text-decoration:underline}</style><link rel="stylesheet" href="/assets/css/dark-mode.css"></head><body><header><div class="nav-container"><a class="logo" href="/" style="visibility:visible">Yiqin Zhao</a><div class="mobile-nav stack" horizontal-layout="space-between" vertical-layout="center"><div class="stack nav-title">blog</div><div class="menu-button" onclick="onMenuButtonClick()"><img id="menu-button-icon" src="/assets/img/menu.svg" alt=""></div></div><div class="stack menu-row hide-menu-row"><a class="menu-item" href="/">home</a> <a class="menu-item" href="/news/">news</a> <a class="menu-item" href="/project/">project</a> <a class="menu-item" href="/publication/">publication</a> <a class="menu-item menu-item-active" href="/blog/">blog</a> <a class="menu-item" href="/assets/yiqinzhao-cv.pdf">CV</a></div></div><script>function onMenuButtonClick(){let e=document.querySelector(".menu-row").classList;e.contains("hide-menu-row")?(e.remove("hide-menu-row"),document.querySelector("#menu-button-icon").setAttribute("src","/assets/img/close.svg")):(e.add("hide-menu-row"),document.querySelector("#menu-button-icon").setAttribute("src","/assets/img/menu.svg"))}</script></header><article class="stack" direction="column"><div class="stack" direction="column"><div class="title">All about Python Performance Optimization</div><div class="stack meta-row" vertical-layout="center"><span class="tag stack" vertical-layout="center"><img class="vector-image" src="/assets/icons/tag.svg" alt=""> Python </span><span class="tag stack" vertical-layout="center"><img class="vector-image" src="/assets/icons/tag.svg" alt=""> Optimization</span><div class="date">Oct 2, 2020</div><div class="eta">ETA: 6min(s)</div></div></div><div class="markdown"><p>Just like the title, this post is all about the <strong>Python performance optimization</strong>. For data science, handling large amount of data is inevitable. Although recently many tools have been developed for developers to accelerate their data science program, achieving good performance is still challenging. For one could easily fall into a performance rabbit hole by just writing one line inappropriately or troubling with squeeze out all the hardware performance. In this post, I will introduce a few handful tips to help you boost your Python data science programs and achieve 10X or more performance improvement.</p><h1 id="%F0%9F%A7%B5%F0%9F%A7%B5%F0%9F%A7%B5-quick-boost-with-multi-threading">üßµüßµüßµ Quick Boost with Multi-threading</h1><p>Modern computer CPU are build with multiple cores while Python program could only use <strong>one core</strong> by default. For some task, replace a <code>for</code> loop with multi-threading code will result in performance improvement. Let's take an example:</p><pre class="hljs"><code><span class="hljs-keyword">import</span> glob

files = glob.glob(<span class="hljs-string">&#x27;./data/*/*.npy&#x27;</span>)
<span class="hljs-comment"># files = [</span>
<span class="hljs-comment">#   &#x27;./data/0/train.npy&#x27;,</span>
<span class="hljs-comment">#   &#x27;./data/0/label.npy&#x27;,</span>
<span class="hljs-comment">#   &#x27;./data/1/train.npy&#x27;,</span>
<span class="hljs-comment">#   &#x27;./data/1/label.npy&#x27;,</span>
<span class="hljs-comment">#   ...</span>
<span class="hljs-comment"># ]</span>

<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> files:
    output = some_operation(f)
    <span class="hljs-comment"># ...</span>
</code></pre><p>The above code apply a function <code>some_operation</code> to some files. While, if you have a large amount of files need to be process, the above code could be slow, a very likely performance bottleneck is the single thread Python feature. While, to apply multi threading to improve your code performance, it requires a little bit change on the original code:</p><pre class="hljs"><code><span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Pool


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">processor</span>(<span class="hljs-params">f</span>):</span>
    output = some_operation(f)

    <span class="hljs-comment"># ...</span>

    <span class="hljs-comment"># If you need the output, you can return it.</span>
    <span class="hljs-comment"># And the returned output will be ordered</span>
    <span class="hljs-comment"># as the input.</span>
    <span class="hljs-keyword">return</span> output

files = glob.glob(<span class="hljs-string">&#x27;./data/*/*.npy&#x27;</span>)

p = Pool(<span class="hljs-number">8</span>) <span class="hljs-comment"># Number of CPU cores</span>
outputs = p.<span class="hljs-built_in">map</span>(processor, files)
</code></pre><p>The above code removed the <code>for</code> loop and replaced it loop body with a function <code>processor</code>, then map all files into multi threads processing through the <code>map</code> function. Here you wanna maximize the utilization of your CPU resources by assign as many threads as your CPU cores. The <code>multiprocessing</code> is a built-in module that came with most python distribution, which means you are likely to use it without pip install.</p><p>Apparently, you might want to ask a question: <strong>when should I use multi-threading?</strong> And it is an important question. To answer this question, we would best to understand why multi threading can improve performance. While if you are not interested in this part or hurry in time, please scroll down to the conclusion of this section.</p><hr><p>üò¥ <em>The boring part...</em></p><p>For the first place, multi threading is like assigning a huge task to multiple person. <strong>How could the original task be broke down</strong> is an important factor to the feasibility of multi threading. If the original task can be break down into multiple <strong>small</strong> and <strong>independent</strong> subtasks, we are likely to use multi threading. Like the above example, the processing of each file is independent to each other and much more lightweight comparing to the whole task.</p><p>I keep using the word <strong>likely</strong> because <strong>small and independent subtasks</strong> is just not enough to release the power of multi threading. You also need to know what takes most time of a single subtask, if the time is taken by CPU as doing some kind of calculation, e.g., do Fourier transform on a audio file, use multiple CPU will greatly improve the performance. While, if most time is consumed on I/O, e.g., reading a huge file, you might get even worse performance by using multi threading, because the psychical storage device need to schedule access order for multiple I/O jobs.</p><p>But what if my task is <strong>not independent to each other</strong>. Fortunately it might not be the end of world to the multi threading. A very commonly seen example is stitching multiple images, in such case, your operation function might take multiple input and generate only one output. This is also a perfect use case for multi threading, because even you have dependencies in your data, the <strong>operation do not depend on other operation results</strong>. This is an important feature, because during multi threading execution, the order of execution to each subtask can not be predicted easily, because if you assign a specific order to your subtasks execution, you will certainly lost some performance, <strong>because some subtask need to wait others</strong>, or even convert your multi threading code into a single threading one. An appropriate example would be the calculation on time serial data, because your current calculation might depend on previous time step. In this case, you are likely going to move your focus from parallelize to parallelize the calculation on each time step, or optimize the calculation itself.</p><hr><p>A good way to monitor your multi threading program is using the command line tool <code>htop</code>. In the most ideal situation, you will see your CPU reaches 100% utilization and being occupied by your program!</p><p><strong>Conclusion.</strong> If your task can be break down into <strong>small</strong>, <strong>independent</strong> and <strong>CPU bound</strong> subtasks, uses the multi threading feature. This method suits in most cases of doing CPU calculation, like <em>feature extraction</em>, <em>image processing</em>. As you can see, this method is easy to use, and can be applied to a lot of scenarios.</p><h1 id="%F0%9F%94%A2-accelerating-matrix-operation">üî¢ Accelerating Matrix Operation</h1><p>Although multi threading boost your code easily, it is not the best way for accelerating matrix operation which is very common in linear algebra tasks. Here we use a intuitive example to explain the reason behind it:</p><pre class="hljs"><code><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

m = np.ones((<span class="hljs-number">10000</span>, <span class="hljs-number">10000</span>))

<span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(m.shape[<span class="hljs-number">0</span>])):
    <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(m.shape[<span class="hljs-number">1</span>])):
        m[v, u] = some_operation(m[v, u])
</code></pre><p>You may try to use multi threading for optimizing the above code, and you will get some performance improvement, but it might not be much even you used all your CPU cores. This is because the major performance bottleneck here is the array access time. Every array access was triggered in the Python environment and reached data with low-level C++ code.</p><p>So, is there any good way to speed up the access? Unfortunately, this could be very difficult if you don't want to do lots of modifications on your existing Python code. But if we rethink the problem, instead of trying to eliminate the performance bottleneck between Python and C++, we can turn everything into fast binary code!</p><p>This idea sounds super crazy, but could be super simple if we use right tools. For example, <a href="http://numba.pydata.org">Numba</a> provides JIT (just-in-time compilation) to turn Python code into compiled binary code. And it has deeply integrated with Numpy ecosystem, that could automatically optimize array access during compilation. Let's look at an example:</p><pre class="hljs"><code><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> numba <span class="hljs-keyword">import</span> njit

<span class="hljs-meta">@njit</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">optimized_func</span>(<span class="hljs-params">m</span>):</span>
    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(m.shape[<span class="hljs-number">0</span>])):
        <span class="hljs-keyword">for</span> u <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(m.shape[<span class="hljs-number">1</span>])):
            m[v, u] = some_operation(m[v, u])

arr = np.ones((<span class="hljs-number">10000</span>, <span class="hljs-number">10000</span>))
optimized_func(arr)
</code></pre><p>This example turns the above python code into an optimized Numba code. As you can see, we use the <code>@njit</code> decorator to optimize a function, and the <code>optimized_func</code> will be complied into binary code before execution.</p><p>There is always a &quot;but&quot;. But since JIT compilation require strict and strong variable type, optimize a large Python program will not be easy. On the other hand, you shouldn't have bottle neck everywhere in your program, they should mostly located in some heavy computation functions. And every external function in <code>njit</code> decorated function should also be decorated.</p><p>We will not go into details of <strong>Numba</strong> optimization in this article, please refer to its documents for more comprehensive explanations. Besides Numba, Cython is also a choice. But personally I would not recommend Cython since it provide less IDE support.</p><h1 id="%F0%9F%90%8E-unleash-the-power-of-gpu">üêé Unleash the Power of GPU</h1><p>For most time, you should be satisfied after using Numba in your code. However, there might be chance that your program has huge amount of linear algebra computation, e.g., a neural network. In this scenario, you could use GPU to boost your performance.</p><p>Before talking about GPU, we should take a look back to our existing optimization skills. A obvious technique that plays an important part is parallel, and it is also the most important feature of GPU. Unlike CPU have few but strong cores, GPU has many but week cores. If you can break down your task into small pieces that can be executed in parallel, execute them on GPU would be much faster than CPU. And this kind of task break can often happens on linear algebra programs that widely used in computer graphics and machine learning.</p><p>Fortunately, you can still use <strong>Numbda</strong> to compile your Python code into CUDA GPU code. And another choice is <a href="https://cupy.chainer.org">CuPy</a>.</p><h1 id="%F0%9F%94%A5-nirvana-rebirth-in-c%2B%2B">üî• Nirvana Rebirth in C++</h1><p><strong>To be continued....</strong></p></div></article><footer class="stack foot" direction="column" horizontal-layout="center"><span>¬© <a href="/">Yiqin Zhao</a> 2021. Last Updated: Feb 2, 2021</span> <span>Proudly Powered by <a href="https://github.com/YiqinZhao/Ploceus">Ploceus</a></span></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/sweetalert/2.1.2/sweetalert.min.js" integrity="sha512-AA1Bzp5Q0K1KanKKmvN/4d3IRKVlv9PYgwFPvm32nPO6QS8yH1HO7LbgB1pgiOxPtfeg5zEn2ba64MUcqJx6CA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/crypto-js/4.0.0/crypto-js.min.js" integrity="sha512-nOQuvD9nKirvxDdvQ9OMqe2dgapbPB7vYAMrzJihw5m+aNcf0dX53m6YxM4LgA9u8e9eg9QX+/+mPu8kCNpV2A==" crossorigin="anonymous"></script><script>window.addEventListener("load",function(){document.querySelectorAll(".encrypted").forEach(e=>{e.addEventListener("click",function(){this.classList.contains("encrypted")&&swal("Password",{content:"input"}).then(t=>{let e=this;for(;!e.classList.contains("markdown");)e=e.parentNode;e.querySelectorAll(".encrypted").forEach(e=>{e.innerHTML=CryptoJS.AES.decrypt(e.innerHTML,t).toString(CryptoJS.enc.Utf8),e.classList.remove("encrypted"),e.classList.add("decrypted")})})})})})</script></body></html>