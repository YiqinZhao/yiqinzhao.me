[{"data":1,"prerenderedAt":813},["ShallowReactive",2],{"$BPopkF1nvN":3,"$QtpWXez6C4":334,"$OToglkIAsf":518},{"id":4,"title":5,"body":6,"description":38,"disableFancyImage":326,"extension":327,"hideTitle":326,"leadingImage":328,"meta":329,"navigation":326,"path":330,"seo":331,"stem":332,"__hash__":333},"page/index.md","Home",{"type":7,"value":8,"toc":322},"minimal",[9,12,32,88,222],[10,11],"index-header",{},[13,14,15],"ultra-wide-row",{},[16,17,23,24,31],"div",{"className":18},[19,20,21,22],"bg-gray-100","p-5","text-red-600","text-justify","\nüîä Super excited to announce that I will be joining ",[25,26,30],"a",{"className":27,"target":28,"href":29},[21],"_blank","https://www.rit.edu/computing/school-interactive-games-and-media","IGM@RIT"," as a tenure-track assistant professor in Fall 2025.\n",[33,34,35,73],"split-block",{},[36,37,39,44,67,70],"template",{"v-slot:left":38},"",[40,41,43],"h2",{"id":42},"Ô∏è-about-me","ü¶∏üèª‚Äç‚ôÇÔ∏è About Me",[45,46,47,48,54,55,60,61,66],"p",{},"I am a final-year Computer Science Ph.D. candidate at ",[25,49,53],{"href":50,"rel":51},"https://wpi.edu",[52],"nofollow","Worcester Polytechnic Institute (WPI)"," and a proud member of ",[25,56,59],{"href":57,"rel":58},"https://cake-lab.github.io",[52],"The Cake Lab"," research group.\nI feel extremely fortunate to be advised by my kind and wise advisor ",[25,62,65],{"href":63,"rel":64},"https://tianguo.info",[52],"Prof. Tian Guo",".",[45,68,69],{},"My research includes building AI models and AI system supports for dynamic application scenarios.\nIn particular, my research pays special interest in exploring the emerging area of immersive computing.\nSpecifically, my works aim to safely and robustly blend computing into physical environments.\nMy work explores problems and solutions that allow mobile computing systems to adapt to the complex dynamics of key mobile computing stakeholders‚Äìuser, device, environment, and AI system.\nCentered around this objective, I have worked on environment sensing and perception systems, context-aware generative AI systems, privacy-preserving AI content generation, and infrastructure projects for AR experimentation.",[45,71,72],{},"In the past, I had the fortunate to work with and study from many awesome industry researchers during my time at Adobe Research, Google AR&VR, Kuaishou Y-tech Graphics AI team, and Baidu.",[36,74,75,79,82],{"v-slot:right":38},[40,76,78],{"id":77},"news","üì∞ News",[80,81],"short-news",{},[45,83,84],{},[25,85,87],{"href":86},"/news/","More news >>>",[33,89,90,160],{},[36,91,92,96,125,146],{"v-slot:left":38},[40,93,95],{"id":94},"education","üè´ Education",[97,98,100],"experience-row",{"icon":99},"wpi.png",[45,101,102,106,109,110,112,113,118,119,122,124],{},[103,104,105],"strong",{},"Worcester Polytechnic Institute, Worcester, MA",[107,108],"br",{},"\nPh.D. student.",[107,111],{},"\nResearch assistant in ",[25,114,117],{"href":115,"rel":116},"https://cake.wpi.edu",[52],"TheCakeLab",", advised by ",[25,120,65],{"href":63,"rel":121},[52],[107,123],{},"\nAug 2021 - Present (Expected: summer 2025)",[97,126,127],{"icon":99},[45,128,129,131,133,134,136,137,118,140,143,145],{},[103,130,105],{},[107,132],{},"\nM.S. student.",[107,135],{},"\nResearch assistant  in ",[25,138,117],{"href":115,"rel":139},[52],[25,141,65],{"href":63,"rel":142},[52],[107,144],{},"\nAug 2019 - June, 2021",[97,147,149],{"icon":148},"tjnu.png",[45,150,151,154,156,157,159],{},[103,152,153],{},"Tianjin Normal University, Tianjin, China",[107,155],{},"\nBachelor of Engineering in Software Engineering",[107,158],{},"\nSept, 2015 - Jun, 2019",[36,161,162,166,180,194,208],{"v-slot:right":38},[40,163,165],{"id":164},"industry-experiences","üë®‚Äçüíª Industry Experiences",[97,167,169],{"icon":168},"adobe.png",[45,170,171,174,176,177,179],{},[103,172,173],{},"Adobe Research, San Jose, CA",[107,175],{},"\nResearch Scientist Intern",[107,178],{},"\nMay 2024 - Aug 2024",[97,181,183],{"icon":182},"google.png",[45,184,185,188,190,191,193],{},[103,186,187],{},"Google, Mountain View, CA",[107,189],{},"\nStudent Researcher",[107,192],{},"\nMay 2022 - May 2023",[97,195,197],{"icon":196},"kuaishou.png",[45,198,199,202,204,205,207],{},[103,200,201],{},"Kuaishou Technology, Palo Alto, CA",[107,203],{},"\nResearch Intern",[107,206],{},"\nJan, 2022 - May, 2022",[97,209,211],{"icon":210},"baidu.png",[45,212,213,216,218,219,221],{},[103,214,215],{},"Baidu, Beijing, China",[107,217],{},"\nSoftware Engineering Intern",[107,220],{},"\nJun, 2018 - Sept, 2018",[13,223,224,228,237,244,251,266,274,281,302,308,316],{},[40,225,227],{"id":226},"selected-publications","üìÑ Selected Publications",[229,230],"publication-row",{":artifactLinks":231,":authors":232,":venue":233,"thumbnail":234,"title":235,"type":236},"{\"arXiv\":\"https://arxiv.org/pdf/2411.02179\"}","[\"Yiqin Zhao\",\"Mallesham Dasri\",\"Tian Guo\"]","{\"acronym\":\"arXiv\",\"year\":2024,\"name\":\"arXiv\"}","clear.png","CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality","demo",[229,238],{":artifactLinks":239,":authors":240,":venue":241,"thumbnail":242,"title":243,"type":236},"{\"Website\":\"https://cake.wpi.edu/ARFlow/\",\"Proceeding\":\"https://dl.acm.org/doi/10.1145/3638550.3643617\",\"Code\":\"https://github.com/cake-lab/ARFlow\"}","[\"Yiqin Zhao\",\"Tian Guo\"]","{\"acronym\":\"HotMobile\",\"year\":2024,\"name\":\"25th International Workshop on Mobile Computing Systems and Applications\"}","arflow.jpeg","Demo: ARFlow: A Framework for Simplifying AR Experimentation Workflow",[229,245],{":artifactLinks":246,":authors":247,":venue":241,"thumbnail":248,"title":249,"type":250},"{\"Proceeding\":\"https://dl.acm.org/doi/10.1145/3638550.3641122\",\"arXiv (extended version)\":\"https://arxiv.org/abs/2310.14437\"}","[\"Ashkan Ganj\",\"Yiqin Zhao\",\"Hang Su\",\"Tian Guo\"]","depth-estimation.png","Mobile AR Depth Estimation: Challenges & Prospects","conference",[229,252,258],{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":250},"{\"Proceeding\":\"https://dl.acm.org/doi/abs/10.1145/3615452.3617941\",\"arXiv\":\"https://arxiv.org/abs/2307.08587\",\"Website\":\"https://cake.wpi.edu/expar/\"}","[\"Ashkan Ganj\",\"Yiqin Zhao\",\"Federico Galbiati\",\"Tian Guo\"]","{\"acronym\":\"ImmerCom\",\"year\":2023,\"name\":\"1st ACM Workshop on Mobile Immersive Computing, Networking, and Systems\"}","expar.png","Toward Scalable and Controllable AR Experimentation",[45,259,260],{},[261,262,265],"span",{"className":263},[21,264],"font-bold","üèÜ Best Paper Runner-up.",[229,267],{":artifactLinks":268,":authors":269,":venue":270,"thumbnail":271,"title":272,"type":273},"{\"Proceeding\":\"https://dl.acm.org/doi/abs/10.1145/3550291\",\"arXiv\":\"https://arxiv.org/pdf/2301.06184.pdf\",\"Slides\":\"https://docs.google.com/presentation/d/1wrHaZorkVvMyE2NENwS43vlrEm2Vt4iaAH3X-wsYBuE/edit?usp=sharing\",\"Code\":\"https://github.com/cake-lab/LitAR\",\"Website\":\"/project/litar/\"}","[\"Yiqin Zhao\",\"Chongyang Ma\",\"Haibin Huang\",\"Tian Guo\"]","{\"acronym\":\"IMWUT\",\"year\":2022,\"name\":\"The Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\"}","litar-ubicomp22.png","LitAR: Visually Coherent Lighting for Mobile Augmented Reality","journal",[229,275],{":artifactLinks":276,":authors":277,":venue":278,"thumbnail":279,"title":280,"type":250},"{\"Proceeding\":\"https://dl.acm.org/doi/abs/10.1145/3503161.3548386\",\"arXiv\":\"https://arxiv.org/abs/2207.03056\",\"Slides\":\"https://docs.google.com/presentation/d/1goYpS9PXr0YLLPQUJcF9P-q8n0iV1A1UvReFDSuV3dk/edit?usp=sharing\",\"Website\":\"/project/privacy-preserving-reflection\"}","[\"Yiqin Zhao\",\"Sheng Wei\",\"Tian Guo\"]","{\"acronym\":\"ACMMM\",\"year\":2022,\"name\":\"30th ACM International Conference on Multimedia\"}","privacy-preserving-reflection.png","Privacy-preserving Reflection Rendering for Augmented Reality",[229,282,287],{":artifactLinks":283,":authors":240,":venue":284,"thumbnail":285,"title":286,"type":250},"{\"Proceeding\":\"https://dl.acm.org/doi/10.1145/3458864.3467886?cid=99659479290\",\"arXiv\":\"https://arxiv.org/abs/2106.15280\",\"Slides\":\"https://drive.google.com/file/d/1iWW6l6XQu_LL-EuA323jecwnVPOa3mWa/view?usp=sharing\",\"Website\":\"/project/xihe/\"}","{\"acronym\":\"MobiSys\",\"year\":2021,\"name\":\"The 19th ACM International Conference on Mobile Systems, Applications, and Services\"}","xihe-mobisys2021.png","Xihe: a 3D Vision-Based Lighting Estimation Framework for Mobile Augmented Reality",[45,288,289],{},[261,290,292,301],{"className":291},[21],[293,294],"img",{"className":295,"src":300},[296,297,298,299],"inline","w-4","my-0","mt-[-0.2em]","/assets/img/icons/artifacts_evaluated_functional_dl.jpg"," Artifacts Evaluated ‚Äì Functional v1.1",[229,303],{":artifactLinks":304,":authors":240,":venue":305,"thumbnail":306,"title":307,"type":250},"{\"Proceeding\":\"https://arxiv.org/abs/2004.00006\",\"arXiv\":\"https://arxiv.org/abs/2004.00006\",\"Slides\":\"https://drive.google.com/file/d/1NUHDf3uxNuXwvqFjXw6BGFADgQdOc9tp/view?usp=sharing\",\"Website\":\"/project/point-ar/\"}","{\"acronym\":\"ECCV\",\"year\":2020,\"name\":\"16th European Conference on Computer Vision\"}","pointar-eccv.png","PointAR: Efficient Lighting Estimation for Mobile Augmented Reality",[229,309],{":artifactLinks":310,":authors":311,":venue":312,"thumbnail":313,"title":314,"type":250,":hideBottomBorder":315},"{\"Proceeding\":\"https://www.isca-speech.org/archive_v0/Interspeech_2018/abstracts/1477.html\",\"Website\":\"/project/deep-spectrum/\"}","[\"Ziping Zhao\",\"Yu Zheng\",\"Zixing Zhang\",\"Haishuai Wang\",\"Yiqin Zhao\",\"Chao Li\"]","{\"acronym\":\"INTERSPEECH\",\"year\":2018,\"name\":\"Annual Conference of the International Speech Communication Association\"}","deep-spectrum-interspeech.png","Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition","true",[45,317,318],{},[25,319,321],{"href":320},"/publication/","Full publication list >>>",{"title":38,"searchDepth":323,"depth":323,"links":324},2,[325],{"id":226,"depth":323,"text":227},true,"md",null,{},"/",{"title":5,"description":38},"index","wzuxf8f9_LYNUNsST_5nYSBZBw51fNQ2AwpDb5TugC0",{"id":4,"title":5,"body":335,"description":38,"disableFancyImage":326,"extension":327,"hideTitle":326,"leadingImage":328,"meta":516,"navigation":326,"path":330,"seo":517,"stem":332,"__hash__":333},{"type":7,"value":336,"toc":513},[337,339,347,378,474],[10,338],{},[13,340,341],{},[16,342,23,344,31],{"className":343},[19,20,21,22],[25,345,30],{"className":346,"target":28,"href":29},[21],[33,348,349,368],{},[36,350,351,353,364,366],{"v-slot:left":38},[40,352,43],{"id":42},[45,354,47,355,54,358,60,361,66],{},[25,356,53],{"href":50,"rel":357},[52],[25,359,59],{"href":57,"rel":360},[52],[25,362,65],{"href":63,"rel":363},[52],[45,365,69],{},[45,367,72],{},[36,369,370,372,374],{"v-slot:right":38},[40,371,78],{"id":77},[80,373],{},[45,375,376],{},[25,377,87],{"href":86},[33,379,380,430],{},[36,381,382,384,402,420],{"v-slot:left":38},[40,383,95],{"id":94},[97,385,386],{"icon":99},[45,387,388,390,109,392,112,394,118,397,400,124],{},[103,389,105],{},[107,391],{},[107,393],{},[25,395,117],{"href":115,"rel":396},[52],[25,398,65],{"href":63,"rel":399},[52],[107,401],{},[97,403,404],{"icon":99},[45,405,406,408,133,410,136,412,118,415,418,145],{},[103,407,105],{},[107,409],{},[107,411],{},[25,413,117],{"href":115,"rel":414},[52],[25,416,65],{"href":63,"rel":417},[52],[107,419],{},[97,421,422],{"icon":148},[45,423,424,426,156,428,159],{},[103,425,153],{},[107,427],{},[107,429],{},[36,431,432,434,444,454,464],{"v-slot:right":38},[40,433,165],{"id":164},[97,435,436],{"icon":168},[45,437,438,440,176,442,179],{},[103,439,173],{},[107,441],{},[107,443],{},[97,445,446],{"icon":182},[45,447,448,450,190,452,193],{},[103,449,187],{},[107,451],{},[107,453],{},[97,455,456],{"icon":196},[45,457,458,460,204,462,207],{},[103,459,201],{},[107,461],{},[107,463],{},[97,465,466],{"icon":210},[45,467,468,470,218,472,221],{},[103,469,215],{},[107,471],{},[107,473],{},[13,475,476,478,480,482,484,491,493,495,505,507,509],{},[40,477,227],{"id":226},[229,479],{":artifactLinks":231,":authors":232,":venue":233,"thumbnail":234,"title":235,"type":236},[229,481],{":artifactLinks":239,":authors":240,":venue":241,"thumbnail":242,"title":243,"type":236},[229,483],{":artifactLinks":246,":authors":247,":venue":241,"thumbnail":248,"title":249,"type":250},[229,485,486],{":artifactLinks":253,":authors":254,":venue":255,"thumbnail":256,"title":257,"type":250},[45,487,488],{},[261,489,265],{"className":490},[21,264],[229,492],{":artifactLinks":268,":authors":269,":venue":270,"thumbnail":271,"title":272,"type":273},[229,494],{":artifactLinks":276,":authors":277,":venue":278,"thumbnail":279,"title":280,"type":250},[229,496,497],{":artifactLinks":283,":authors":240,":venue":284,"thumbnail":285,"title":286,"type":250},[45,498,499],{},[261,500,502,301],{"className":501},[21],[293,503],{"className":504,"src":300},[296,297,298,299],[229,506],{":artifactLinks":304,":authors":240,":venue":305,"thumbnail":306,"title":307,"type":250},[229,508],{":artifactLinks":310,":authors":311,":venue":312,"thumbnail":313,"title":314,"type":250,":hideBottomBorder":315},[45,510,511],{},[25,512,321],{"href":320},{"title":38,"searchDepth":323,"depth":323,"links":514},[515],{"id":226,"depth":323,"text":227},{},{"title":5,"description":38},{"id":519,"title":520,"body":521,"description":38,"disableFancyImage":326,"extension":327,"hideTitle":807,"leadingImage":808,"meta":809,"navigation":326,"path":810,"seo":811,"stem":77,"__hash__":812},"page/news.md","News",{"type":7,"value":522,"toc":805},[523,527],[524,525],"markdown-header",{"subtitle":526,"title":520},"üì¢ I will be joining IGM@RIT as a tenure track assistant professor in Fall 2025!",[528,529,530,537,543,555,567,577,591,603,609,614,625,636,646,654,660,667,684,695,708,713,727,733,739,745,751,757,763,769,775,781,787,793,799],"ul",{},[531,532,533,536],"li",{},[103,534,535],{},"03/27/2025"," üéâ I will be joining IGM@RIT as a tenure track assistant professor in Fall 2025!",[531,538,539,542],{},[103,540,541],{},"08/11/2024"," üéâ One paper accepted at ImmerCom 2024!",[531,544,545,548,549,554],{},[103,546,547],{},"05/13/2024"," üéâ I joined ",[25,550,553],{"href":551,"rel":552},"http://www.chongyangma.com/team/index.html",[52],"Adobe Research"," as a research scientist intern.",[531,556,557,560,561,566],{},[103,558,559],{},"03/01/2024"," üéâ Our demo ",[25,562,565],{"href":563,"rel":564},"https://cake.wpi.edu/ARFlow/",[52],"ARFlow: A Framework for Simplifying AR Experimentation Workflow"," has been accepted by HotMobile 2024!",[531,568,569,572,573,566],{},[103,570,571],{},"02/17/2024"," üéâ Our paper ",[25,574,249],{"href":575,"rel":576},"https://arxiv.org/pdf/2310.14437",[52],[531,578,579,582,583,586,587,590],{},[103,580,581],{},"06/10/2023"," üèÜ Our paper, ",[584,585,257],"em",{},", received ",[103,588,589],{},"best paper runner-up"," award at ImmerCom'23.",[531,592,593,596,597,602],{},[103,594,595],{},"03/31/2023"," üéâ We have released the ",[25,598,601],{"href":599,"rel":600},"https://github.com/cake-lab/LitAR",[52],"code"," of our UbiComp 2022 paper LitAR.",[531,604,605,608],{},[103,606,607],{},"12/15/2022"," üéâ Received student travel grant from HotMobile 2023, thank you!",[531,610,611,613],{},[103,612,607],{}," üéâ Passed my classes and research qualifications, I'm a Ph.D. candidate now!",[531,615,616,572,619,624],{},[103,617,618],{},"12/09/2022",[25,620,623],{"href":621,"rel":622},"https://arxiv.org/pdf/2301.06143",[52],"Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality"," has been accepted by HotMobile 2023!",[531,626,627,630,631,635],{},[103,628,629],{},"01/18/2022"," üéâ I joined the ",[25,632,634],{"href":551,"rel":633},[52],"Y-tech Graphics AI team"," as a research intern.",[531,637,638,572,641,645],{},[103,639,640],{},"05/17/2022",[25,642,644],{"href":643},"/project/litar","LitAR"," has been accepted by UbiComp 2022!",[531,647,648,572,650,653],{},[103,649,640],{},[25,651,280],{"href":652},"/project/privacy-preserving-reflection"," has been accepted by ACM MM 2022!",[531,655,656,659],{},[103,657,658],{},"03/04/2022"," üéâ I will join Google as a research intern in the summer!",[531,661,662,630,664,635],{},[103,663,629],{},[25,665,634],{"href":551,"rel":666},[52],[531,668,669,672,673,677,678,683],{},[103,670,671],{},"05/24/2021"," ‚ú® We just released the ",[25,674,676],{"href":675},"/project/point-ar","Xihe"," source code! Check our ",[25,679,682],{"href":680,"rel":681},"https://github.com/cake-lab/Xihe",[52],"GitHub"," repo.",[531,685,686,689,690,694],{},[103,687,688],{},"04/15/2021"," üéâ I will join the ",[25,691,634],{"href":692,"rel":693},"http://chongyangma.com/team/index.html",[52]," as a research intern in next spring!",[531,696,697,699,700,704,705,66],{},[103,698,688],{}," üéâ I'm thrilled to announce that I will continue my Ph.D. study at ",[25,701,59],{"href":702,"rel":703},"https://cake.wpi.edu/",[52]," with ",[25,706,65],{"href":63,"rel":707},[52],[531,709,710,712],{},[103,711,688],{}," üéâ I've presented my M.S. Thesis.",[531,714,715,718,719,722,723,683],{},[103,716,717],{},"04/03/2021"," ‚ú® ",[25,720,721],{"href":675},"PointAR"," source code is now released! Check our ",[25,724,682],{"href":725,"rel":726},"https://github.com/cake-lab/PointAR",[52],[531,728,729,732],{},[103,730,731],{},"03/25/2021"," üî• New paper on mobile system for 3D vision-based lighting estimation has been conditionally accepted to MobiSys 2021.",[531,734,735,738],{},[103,736,737],{},"08/25/2020"," üéôÔ∏è I presented our efficient mobile AR lighting estimation paper at ECCV 2020.",[531,740,741,744],{},[103,742,743],{},"07/22/2020"," üî• Our paper on efficient mobile AR lighting estimation neural network has been accepted to ECCV 2020.",[531,746,747,750],{},[103,748,749],{},"03/03/2020"," üéôÔ∏è I presented my poster at the HotMobile 2020 conference in Austin, TX.",[531,752,753,756],{},[103,754,755],{},"02/01/2020"," üî• Our poster on mobile AR lighting estimation has been accepted to HotMobile 2020.",[531,758,759,762],{},[103,760,761],{},"08/26/2019"," ü¶∏üèª‚Äç‚ôÇÔ∏è I joined Worcester Polytechnic Institute CakeLab.",[531,764,765,768],{},[103,766,767],{},"10/26/2018"," üéôÔ∏è I presented our paper on ASMMC-MMAC 2018 workshop.",[531,770,771,774],{},[103,772,773],{},"10/15/2018"," üèÜ I received Wang Kechang Technology innovation scholarship (\u003C1%).",[531,776,777,780],{},[103,778,779],{},"10/13/2018"," üèÜ I received annual special scholarship at Tianjin Normal University (top 5%).",[531,782,783,786],{},[103,784,785],{},"08/15/2018"," üî• Our paper about human speech emotion in spectrogram representation has submitted to ACM Multimedia 2018 ASMMC-MMAC workshop!",[531,788,789,792],{},[103,790,791],{},"07/04/2018"," üë®üèª‚Äçüíª I joined the DuerOS department at Baidu, Inc as a frontend software engineer intern.",[531,794,795,798],{},[103,796,797],{},"09/30/2017"," üèÜ I received third prize of the 2017 China national Mobile Innovation Contest (top 6%).",[531,800,801,804],{},[103,802,803],{},"09/30/2016"," üèÜ I received third prize of the 2016 China national Mobile Innovation Contest (top 10%).",{"title":38,"searchDepth":323,"depth":323,"links":806},[],false,"me-news-google.png",{},"/news",{"title":520,"description":38},"YieWpTWS3LAYmnbW6rtEvdg83M4iMurS12FST-uPIM0",1743195575454]