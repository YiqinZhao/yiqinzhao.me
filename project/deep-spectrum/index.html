<!DOCTYPE html>
<html >
<head><meta charset="utf-8">
<title>Deep Spectrum Feature Representations for Speech Emotion Recognition - Yiqin Zhao</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="icon" type="image/x-icon" href="/site-icons/favicon.ico">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.0/dist/katex.min.css">
<meta name="og:title" content="Deep Spectrum Feature Representations for Speech Emotion Recognition"><link rel="modulepreload" href="/project/deep-spectrum/_payload.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/entry.d85a4529.js"><link rel="preload" as="style" href="/_nuxt/entry.0d6ce1d7.css"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/document-driven.6e49f9c7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DocumentDrivenEmpty.d7375076.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRenderer.8604f978.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentRendererMarkdown.aeb4793c.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/index.a6ef77ff.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DocumentDrivenNotFound.9b310950.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/head.8c94056a.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/default.8d670c31.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Navigator.b794c455.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/qin-logo.d1f3a97d.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentDoc.a13db5df.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ContentQuery.39a448cb.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/asyncData.92a39242.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Footer.0d7cbc0a.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/MarkdownHeader.7828b21e.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseH2.ef74261f.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseP.0b5d795b.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/_plugin-vue_export-helper.c27b6911.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseStrong.e42423de.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseA.715696e3.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/nuxt-link.f03ae30d.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseImg.b8b87e5a.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseCode.c37abd21.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ProseHr.10c0999c.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/cards.1a58639d.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/ContentList.4a87723f.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/client-db.a7d56174.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/error-component.393ef043.js"><link rel="stylesheet" href="/_nuxt/entry.0d6ce1d7.css"><style>body{--tw-bg-opacity:1;background-color:#fafaf9;background-color:rgb(250 250 249/var(--tw-bg-opacity))}.dark body{--tw-bg-opacity:1;background-color:#1c1917;background-color:rgb(28 25 23/var(--tw-bg-opacity))}p{line-height:1.6em;text-align:justify;text-justify:inter-word}a{text-decoration-color:rgba(0,0,0,.4)!important;transition:opacity .1s linear}a:hover{opacity:.4}h2 a{font-weight:900!important;text-decoration:none!important}h2 a:before{border-top:1px solid #000;content:"";display:block;max-width:100%;padding-bottom:.2em;width:100px}</style><style>pre code .line{display:block;min-height:1rem}</style><script>"use strict";const w=window,de=document.documentElement,knownColorSchemes=["dark","light"],preference=window.localStorage.getItem("nuxt-color-mode")||"system";let value=preference==="system"?getColorScheme():preference;const forcedColorMode=de.getAttribute("data-color-mode-forced");forcedColorMode&&(value=forcedColorMode),addColorScheme(value),w["__NUXT_COLOR_MODE__"]={preference,value,getColorScheme,addColorScheme,removeColorScheme};function addColorScheme(e){const o="--"+e+"",t="";de.classList?de.classList.add(o):de.className+=" "+o,t&&de.setAttribute("data-"+t,e)}function removeColorScheme(e){const o="--"+e+"",t="";de.classList?de.classList.remove(o):de.className=de.className.replace(new RegExp(o,"g"),""),t&&de.removeAttribute("data-"+t)}function prefersColorScheme(e){return w.matchMedia("(prefers-color-scheme"+e+")")}function getColorScheme(){if(w.matchMedia&&prefersColorScheme("").media!=="not all"){for(const e of knownColorSchemes)if(prefersColorScheme(":"+e).matches)return e}return"light"}
</script></head>
<body ><div id="__nuxt"><!--[--><!----><!----><!--[--><div class="document-driven-page"><!--[--><!--[--><!--[--><div class="hidden md:block text-normal px-4 py-2 text-gray-500 dark:text-gray-400 mx-auto bg-gray-100 dark:bg-gray-800"><div class="md:flex justify-between max-w-6xl mx-auto"><a class="text-xl font-bold hover:dark:text-white hover:text-black transition-colors" href="/"><span class=""><img class="inline w-10 dark:invert transition-opacity opacity-60 hover:opacity-100" src="/assets/img/qin-logo.svg" alt=""></span></a><div class="flex flex-row justify-center"><!--[--><a class="pl-10 font-medium hover:dark:text-white hover:text-black transition-colors flex items-center" href="/"><span class="transition-border hover:border-b-2 border-b-gray-400">Home</span></a><a class="pl-10 font-medium hover:dark:text-white hover:text-black transition-colors flex items-center" href="/news/"><span class="transition-border hover:border-b-2 border-b-gray-400">News</span></a><a class="pl-10 font-medium hover:dark:text-white hover:text-black transition-colors flex items-center" href="/research/"><span class="transition-border hover:border-b-2 border-b-gray-400">Research</span></a><a class="pl-10 font-medium hover:dark:text-white hover:text-black transition-colors flex items-center" href="/project/"><span class="transition-border text-black dark:text-white border-b-2 border-b-gray-400">Projects</span></a><a class="pl-10 font-medium hover:dark:text-white hover:text-black transition-colors flex items-center" href="/yiqinzhao-cv.pdf"><span class="transition-border hover:border-b-2 border-b-gray-400">CV</span></a><!--]--></div></div></div><div class="fixed flex md:hidden flex-row justify-between p-5 text-black dark:text-white bg-gray-100 dark:bg-gray-800 z-30 w-full top-0"><span class="text-xl font-bold">Projects</span><span><img class="inline dark:invert" src="/assets/img/icons/menu.svg" alt=""></span></div><div class="flex md:hidden opacity-0 flex-row justify-between p-5 text-black dark:text-white bg-gray-100 dark:bg-gray-800 z-30 w-full top-0"><span class="text-xl font-bold">Projects</span><span><img class="inline dark:invert" src="/assets/img/icons/menu.svg" alt=""></span></div><div class="translate-y-[-100vh] fixed w-full h-full top-0 left-0 z-20 bg-black transform-gpu transition-opacity opacity-0"></div><div class="translate-y-[-100vh] fixed flex flex-col justify-between w-full h-full top-0 left-0 bg-gray-100 dark:bg-gray-800 z-20 transform-gpu transition-transform duration-700"><div class="flex flex-col p-5 pt-24 text-xl text-gray-400 dark:text-gray-500"><!--[--><a class="hover:text-white transition-color py-3" href="/"><div class="flex justify-between"><span class="transition-border hover:border-b-4 border-b-gray-400">Home</span><img class="dark:invert w-6 opacity-50" src="/assets/img/icons/arrow-right-bold.svg" alt=""></div></a><a class="hover:text-white transition-color py-3" href="/news/"><div class="flex justify-between"><span class="transition-border hover:border-b-4 border-b-gray-400">News</span><img class="dark:invert w-6 opacity-50" src="/assets/img/icons/arrow-right-bold.svg" alt=""></div></a><a class="hover:text-white transition-color py-3" href="/research/"><div class="flex justify-between"><span class="transition-border hover:border-b-4 border-b-gray-400">Research</span><img class="dark:invert w-6 opacity-50" src="/assets/img/icons/arrow-right-bold.svg" alt=""></div></a><a class="hover:text-white transition-color py-3" href="/project/"><div class="flex justify-between"><span class="text-black dark:text-white font-medium">Projects</span><img class="opacity-100 dark:invert w-6 opacity-50" src="/assets/img/icons/arrow-right-bold.svg" alt=""></div></a><a class="hover:text-white transition-color py-3" href="/yiqinzhao-cv.pdf"><div class="flex justify-between"><span class="transition-border hover:border-b-4 border-b-gray-400">CV</span><img class="dark:invert w-6 opacity-50" src="/assets/img/icons/arrow-right-bold.svg" alt=""></div></a><!--]--></div><div class="text-gray-400 dark:text-gray-500 text-center py-4">Yiqin Zhao</div></div><!--]--><article class="[&amp;_p_img]:md:max-w-[140%] [&amp;_p_img]:md:mx-[-20%] pb-16 min-h-[100vh] [&amp;_header]:md:max-w-[140%] [&amp;_header]:md:mx-[-20%] [&amp;_pre]:md:text-xs font-serif"><!--[--><div class="prose dark:prose-invert mx-auto p-4 md:p-0 md:text-xl [&amp;_h1]:mt-12"><!--[--><h1 class="dark:text-white text-center text-4xl md:text-5xl font-bold mb-0 mt-0 md:mt-10 max-w-5xl mx-auto">Deep Spectrum Feature Representations for Speech Emotion Recognition</h1><!----><!--]--><h2 id="deep-spectrum-feature-representations-for-speech-emotion-recognition"><a href="#deep-spectrum-feature-representations-for-speech-emotion-recognition"><!--[-->Deep Spectrum Feature Representations for Speech Emotion Recognition<!--]--></a></h2><p><!--[-->Zhao, Ziping and <strong><!--[-->Zhao, Yiqin<!--]--></strong> and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao<!--]--></p><p><!--[-->Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data (<strong><!--[-->ASMMC-MMAC&#39;18<!--]--></strong>)<!--]--></p><p><!--[--><a href="https://dl.acm.org/doi/10.1145/3267935.3267948" rel="nofollow"><!--[--><strong><!--[--><img src="/assets/img/project/deep-spectrum/thumbnail-asmmc-18.png" alt><!--]--></strong><!--]--></a><!--]--></p><!--[--><pre><code><span class="line"><span class="ct-11e560">@inproceedings</span><span class="ct-fb4c3f">{</span><span class="ct-c3f334">deep_spectrum_2018</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">author</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Zhao, Ziping and Zhao, Yiqin and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">title</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Deep Spectrum Feature Representations for Speech Emotion Recognition</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">booktitle</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia</span></span><span class="line"><span class="ct-fb4c3f">    Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">series</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">ASMMC-MMAC&#39;18</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">year</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">2018</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">isbn</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">978-1-4503-5985-6</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">location</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Seoul, Republic of Korea</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">pages</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">27--33</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">numpages</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">7</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">url</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">http://doi.acm.org/10.1145/3267935.3267948</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">doi</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">10.1145/3267935.3267948</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">acmid</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">3267948</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">publisher</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">ACM</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">address</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">New York, NY, USA</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">    </span><span class="ct-c3f334">keywords</span><span class="ct-fb4c3f"> </span><span class="ct-11e560">=</span><span class="ct-fb4c3f"> </span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">attention mechanism, bidirectional long short-term memory,</span></span><span class="line"><span class="ct-fb4c3f">    fully convolutional networks, spectrogram representation, speech emotion recognition</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">}</span></span></code></pre><!--]--><hr><h2 id="exploring-deep-spectrum-representations-via-attention-based-recurrent-and-convolutional-neural-networks-for-speech-emotion-recognition"><a href="#exploring-deep-spectrum-representations-via-attention-based-recurrent-and-convolutional-neural-networks-for-speech-emotion-recognition"><!--[-->Exploring Deep Spectrum Representations via Attention-Based Recurrent and Convolutional Neural Networks for Speech Emotion Recognition<!--]--></a></h2><p><!--[-->Ziping Zhao, Zhongtian Bao, <strong><!--[-->Yiqin Zhao<!--]--></strong>, Zixing Zhang, Nicholas Cummins, Zhao Ren, Björn Schuller<!--]--></p><p><!--[--><strong><!--[-->IEEE Access 2019<!--]--></strong><!--]--></p><p><!--[--><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762126" rel="nofollow"><!--[--><strong><!--[--><img src="/assets/img/project/deep-spectrum/thumbnail-ieee-19.png" alt><!--]--></strong><!--]--></a><!--]--></p><!--[--><pre><code><span class="line"><span class="ct-11e560">@article</span><span class="ct-fb4c3f">{</span><span class="ct-c3f334">zhao2019exploring</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">title</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Exploring deep spectrum representations via attention-based recurrent and</span></span><span class="line"><span class="ct-fb4c3f">  convolutional neural networks for speech emotion recognition</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">author</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span></span><span class="line"><span class="ct-fb4c3f">    Zhao, Ziping and Bao, Zhongtian and Zhao, Yiqin and Zhang, Zixing and Cummins,</span></span><span class="line"><span class="ct-fb4c3f">    Nicholas and Ren, Zhao and Schuller, Bj{\&quot;o}rn</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">journal</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">IEEE Access</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">volume</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">7</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">pages</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">97515--97525</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">year</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">2019</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">publisher</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">IEEE</span><span class="ct-aee26f">}</span></span><span class="line"><span class="ct-fb4c3f">}</span></span></code></pre><!--]--><hr><h2 id="exploring-spatio-temporal-representations-by-integrating-attention-based-bidirectional-lstm-rnns-and-fcns-for-speech-emotion-recognition"><a href="#exploring-spatio-temporal-representations-by-integrating-attention-based-bidirectional-lstm-rnns-and-fcns-for-speech-emotion-recognition"><!--[-->Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition<!--]--></a></h2><p><!--[-->Ziping Zhao, Yu Zheng, Zixing Zhang, Haishuai Wang, <strong><!--[-->Yiqin Zhao<!--]--></strong>, Chao Li.<!--]--></p><p><!--[-->Annual Conference of the International Speech Communication Association, <strong><!--[-->INTERSPEECH 2018<!--]--></strong><!--]--></p><p><!--[--><a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1477.pdf" rel="nofollow"><!--[--><strong><!--[--><img src="/assets/img/project/deep-spectrum/thumbnail-interspeech-18.png" alt><!--]--></strong><!--]--></a><!--]--></p><!--[--><pre><code><span class="line"><span class="ct-11e560">@article</span><span class="ct-fb4c3f">{</span><span class="ct-c3f334">zhao2018exploring</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">title</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Exploring spatio-temporal representations by integrating</span></span><span class="line"><span class="ct-fb4c3f">  attention-based bidirectional-LSTM-RNNs and FCNs for speech emotion recognition</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">author</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">Zhao, Ziping and Zheng, Yu and Zhang, Zixing and Wang, Haishuai and Zhao, Yiqin and Li, Chao</span><span class="ct-aee26f">}</span><span class="ct-fb4c3f">,</span></span><span class="line"><span class="ct-fb4c3f">  </span><span class="ct-c3f334">year</span><span class="ct-11e560">=</span><span class="ct-aee26f">{</span><span class="ct-fb4c3f">2018</span><span class="ct-aee26f">}</span></span><span class="line"><span class="ct-fb4c3f">}</span></span></code></pre><!--]--><h2 id="contact"><a href="#contact"><!--[-->Contact<!--]--></a></h2><p><!--[-->If you have any questions, please feel free to contact <a href="mailto:yiqinzhao@outlook.com" rel="noopener noreferrer"><!--[-->Yiqin Zhao<!--]--></a>.<!--]--></p><style>.ct-aee26f{color:#E9F284}
.ct-c3f334{color:#8BE9FD}
.ct-fb4c3f{color:#F8F8F2}
.ct-11e560{color:#FF79C6}</style></div><!--]--></article><div class="bg-gray-100 dark:bg-gray-800 py-4"><div class="prose mx-auto dark:prose-invert p-4 md:text-lg opacity-60"><div class="flex-col md:flex-row flex justify-between border-b-2 border-b-gray-300"><div><p class="text-sm"> 赵一勤 | Yiqin (Pronunciation: Yi-Chin) <br> CS Ph.D. Candidate <a href="https://cake.wpi.edu">@wpicakelab</a> <br> Student Researcher <a href="https//arvr.google.com">@GoogleVRAR</a> <br> Research Interests: Mobile System, AR, CV, CG </p></div><div><p class="text-sm md:text-right [&amp;_br]:hidden [&amp;_br]:md:inline"><a href="mailto:yiqinzhao@outlook.com">E-Mail</a> <br><a href="https://github.com/YiqinZhao">GitHub</a> <br><a href="https://twitter.com/yiqin_zhao">Twitter</a> <br><a href="https://www.instagram.com/yiqinzhao1996">Instagram</a> <br></p></div></div><div class="text-gray-500 dark:text-gray-400 mx-auto text-sm"><p> © <a href="https://yiqinzhao.me">Yiqin Zhao</a> 2023. Last updated: 2/26/2023. Use my website <a class="underline" href="https://github.com/YiqinZhao/yiqinzhao.me-source">template</a>. </p></div></div></div><!--]--><!--]--></div><!--]--><!--]--></div><script type="module">import p from "/project/deep-spectrum/_payload.js";window.__NUXT__={...p,...((function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag){return {state:{"$scolor-mode":{preference:N,value:N,unknown:u,forced:p},"$sdd-pages":{"/project/deep-spectrum":{_path:O,_dir:B,_draft:p,_partial:p,_locale:n,_empty:p,title:v,description:n,date:"2018-06-12T00:00:00.000Z",thumbnail:"\u002Fassets\u002Fimg\u002Fproject\u002Fdeep-spectrum\u002Fthumbnail.png",tag:P,layout:s,body:{type:"root",children:[{type:a,tag:"markdown-header",props:{title:v},children:[]},{type:a,tag:y,props:{id:Q},children:[{type:c,value:v}]},{type:a,tag:q,props:{},children:[{type:c,value:"Zhao, Ziping and "},{type:a,tag:t,props:{},children:[{type:c,value:"Zhao, Yiqin"}]},{type:c,value:" and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao"}]},{type:a,tag:q,props:{},children:[{type:c,value:"Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data ("},{type:a,tag:t,props:{},children:[{type:c,value:R}]},{type:c,value:")"}]},{type:a,tag:q,props:{},children:[{type:a,tag:z,props:{href:"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3267935.3267948",rel:[C]},children:[{type:a,tag:t,props:{},children:[{type:a,tag:D,props:{alt:n,src:"\u002Fassets\u002Fimg\u002Fproject\u002Fdeep-spectrum\u002Fthumbnail-asmmc-18.png"},children:[]}]}]}]},{type:a,tag:w,props:{code:"@inproceedings{deep_spectrum_2018,\n    author = {Zhao, Ziping and Zhao, Yiqin and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao},\n    title = {Deep Spectrum Feature Representations for Speech Emotion Recognition},\n    booktitle = {Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia\n    Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data},\n    series = {ASMMC-MMAC'18},\n    year = {2018},\n    isbn = {978-1-4503-5985-6},\n    location = {Seoul, Republic of Korea},\n    pages = {27--33},\n    numpages = {7},\n    url = {http:\u002F\u002Fdoi.acm.org\u002F10.1145\u002F3267935.3267948},\n    doi = {10.1145\u002F3267935.3267948},\n    acmid = {3267948},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n    keywords = {attention mechanism, bidirectional long short-term memory,\n    fully convolutional networks, spectrogram representation, speech emotion recognition},\n}\n",language:A,meta:E},children:[{type:a,tag:F,props:{},children:[{type:a,tag:w,props:{__ignoreMap:n},children:[{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:h},children:[{type:c,value:"@inproceedings"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"deep_spectrum_2018"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:G}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"Zhao, Ziping and Zhao, Yiqin and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:H}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:v}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"booktitle"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia"}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:"    Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"series"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:R}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:I}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:S}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"isbn"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"978-1-4503-5985-6"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"location"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"Seoul, Republic of Korea"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:T}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"27--33"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"numpages"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:U}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"url"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"http:\u002F\u002Fdoi.acm.org\u002F10.1145\u002F3267935.3267948"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"doi"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"10.1145\u002F3267935.3267948"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"acmid"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"3267948"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:V}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"ACM"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"address"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"New York, NY, USA"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:o}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"keywords"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:g}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"attention mechanism, bidirectional long short-term memory,"}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:"    fully convolutional networks, spectrogram representation, speech emotion recognition"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:k}]}]}]}]}]},{type:a,tag:W,props:{},children:[]},{type:a,tag:y,props:{id:X},children:[{type:c,value:Y}]},{type:a,tag:q,props:{},children:[{type:c,value:"Ziping Zhao, Zhongtian Bao, "},{type:a,tag:t,props:{},children:[{type:c,value:J}]},{type:c,value:", Zixing Zhang, Nicholas Cummins, Zhao Ren, Björn Schuller"}]},{type:a,tag:q,props:{},children:[{type:a,tag:t,props:{},children:[{type:c,value:"IEEE Access 2019"}]}]},{type:a,tag:q,props:{},children:[{type:a,tag:z,props:{href:"https:\u002F\u002Fieeexplore.ieee.org\u002Fstamp\u002Fstamp.jsp?arnumber=8762126",rel:[C]},children:[{type:a,tag:t,props:{},children:[{type:a,tag:D,props:{alt:n,src:"\u002Fassets\u002Fimg\u002Fproject\u002Fdeep-spectrum\u002Fthumbnail-ieee-19.png"},children:[]}]}]}]},{type:a,tag:w,props:{code:"@article{zhao2019exploring,\n  title={Exploring deep spectrum representations via attention-based recurrent and\n  convolutional neural networks for speech emotion recognition},\n  author={\n    Zhao, Ziping and Bao, Zhongtian and Zhao, Yiqin and Zhang, Zixing and Cummins,\n    Nicholas and Ren, Zhao and Schuller, Bj{\\\"o}rn},\n  journal={IEEE Access},\n  volume={7},\n  pages={97515--97525},\n  year={2019},\n  publisher={IEEE}\n}\n",language:A,meta:E},children:[{type:a,tag:F,props:{},children:[{type:a,tag:w,props:{__ignoreMap:n},children:[{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:h},children:[{type:c,value:Z}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"zhao2019exploring"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:H}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"Exploring deep spectrum representations via attention-based recurrent and"}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:"  convolutional neural networks for speech emotion recognition"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:G}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:"    Zhao, Ziping and Bao, Zhongtian and Zhao, Yiqin and Zhang, Zixing and Cummins,"}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:"    Nicholas and Ren, Zhao and Schuller, Bj{\\\"o}rn"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"journal"}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"IEEE Access"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"volume"}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:U}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:T}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"97515--97525"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:I}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"2019"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:V}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"IEEE"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:k}]}]}]}]}]},{type:a,tag:W,props:{},children:[]},{type:a,tag:y,props:{id:_},children:[{type:c,value:$}]},{type:a,tag:q,props:{},children:[{type:c,value:"Ziping Zhao, Yu Zheng, Zixing Zhang, Haishuai Wang, "},{type:a,tag:t,props:{},children:[{type:c,value:J}]},{type:c,value:", Chao Li."}]},{type:a,tag:q,props:{},children:[{type:c,value:"Annual Conference of the International Speech Communication Association, "},{type:a,tag:t,props:{},children:[{type:c,value:"INTERSPEECH 2018"}]}]},{type:a,tag:q,props:{},children:[{type:a,tag:z,props:{href:"https:\u002F\u002Fwww.isca-speech.org\u002Farchive\u002FInterspeech_2018\u002Fpdfs\u002F1477.pdf",rel:[C]},children:[{type:a,tag:t,props:{},children:[{type:a,tag:D,props:{alt:n,src:"\u002Fassets\u002Fimg\u002Fproject\u002Fdeep-spectrum\u002Fthumbnail-interspeech-18.png"},children:[]}]}]}]},{type:a,tag:w,props:{code:"@article{zhao2018exploring,\n  title={Exploring spatio-temporal representations by integrating\n  attention-based bidirectional-LSTM-RNNs and FCNs for speech emotion recognition},\n  author={Zhao, Ziping and Zheng, Yu and Zhang, Zixing and Wang, Haishuai and Zhao, Yiqin and Li, Chao},\n  year={2018}\n}\n",language:A,meta:E},children:[{type:a,tag:F,props:{},children:[{type:a,tag:w,props:{__ignoreMap:n},children:[{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:h},children:[{type:c,value:Z}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:"zhao2018exploring"}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:H}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"Exploring spatio-temporal representations by integrating"}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:"  attention-based bidirectional-LSTM-RNNs and FCNs for speech emotion recognition"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:G}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:"Zhao, Ziping and Zheng, Yu and Zhang, Zixing and Wang, Haishuai and Zhao, Yiqin and Li, Chao"}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:l}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:r}]},{type:a,tag:b,props:{class:j},children:[{type:c,value:I}]},{type:a,tag:b,props:{class:h},children:[{type:c,value:m}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:i}]},{type:a,tag:b,props:{class:d},children:[{type:c,value:S}]},{type:a,tag:b,props:{class:e},children:[{type:c,value:k}]}]},{type:a,tag:b,props:{class:f},children:[{type:a,tag:b,props:{class:d},children:[{type:c,value:k}]}]}]}]}]},{type:a,tag:y,props:{id:aa},children:[{type:c,value:ab}]},{type:a,tag:q,props:{},children:[{type:c,value:"If you have any questions, please feel free to contact "},{type:a,tag:z,props:{href:"mailto:yiqinzhao@outlook.com"},children:[{type:c,value:J}]},{type:c,value:"."}]},{type:a,tag:"style",children:[{type:c,value:".ct-aee26f{color:#E9F284}\n.ct-c3f334{color:#8BE9FD}\n.ct-fb4c3f{color:#F8F8F2}\n.ct-11e560{color:#FF79C6}"}]}],toc:{title:n,searchDepth:x,depth:x,links:[{id:Q,depth:x,text:v},{id:X,depth:x,text:Y},{id:_,depth:x,text:$},{id:aa,depth:x,text:ab}]}},_type:K,_id:"content:project:deep-spectrum.md",_source:L,_file:"project\u002Fdeep-spectrum.md",_extension:M}},"$sdd-surrounds":{"/project/deep-spectrum":[{_path:ac,_dir:B,_draft:p,_partial:p,_locale:n,_empty:p,title:ad,description:n,date:"2017-05-03T00:00:00.000Z",thumbnail:"\u002Fassets\u002Fimg\u002Fproject\u002F2048\u002Fthumbnail.png",tag:"opensource",layout:s,_type:K,_id:"content:project:2048.md",_source:L,_file:"project\u002F2048.md",_extension:M},{_path:ae,_dir:B,_draft:p,_partial:p,_locale:n,_empty:p,title:af,description:n,date:"2022-07-29T00:00:00.000Z",thumbnail:"\u002Fassets\u002Fimg\u002Fproject\u002Flitar\u002Fthumbnail.png",previewCardDirection:"horizontal",tag:P,layout:s,_type:K,_id:"content:project:litar.md",_source:L,_file:"project\u002Flitar.md",_extension:M}]},"$sdd-globals":{},"$sdd-navigation":[{title:"Home",_path:ag},{title:"News",_path:"\u002Fnews"},{title:"Project",_path:"\u002Fproject",children:[{title:ad,_path:ac,layout:s},{title:v,_path:O,layout:s},{title:af,_path:ae,layout:s},{title:"PointAR: Efficient Lighting Estimation for Mobile Augmented Reality",_path:"\u002Fproject\u002Fpoint-ar",layout:s},{title:"Privacy-preserving Reflection Rendering for Augmented Reality",_path:"\u002Fproject\u002Fprivacy-preserving-reflection",layout:s},{title:"Xihe: A 3D Vision based Lighting Estimation for Mobile AR",_path:"\u002Fproject\u002Fxihe",layout:s}],layout:"cards"},{title:"Research",_path:"\u002Fresearch"}]},_errors:{},serverRendered:u,config:{public:{content:{locales:[],defaultLocale:n,integrity:1677452144303,experimental:{stripQueryParameters:p,clientDB:p},api:{baseURL:"\u002Fapi\u002F_content"},navigation:{fields:["navTitle","layout"]},tags:{p:"prose-p",a:"prose-a",blockquote:"prose-blockquote","code-inline":"prose-code-inline",code:"prose-code",em:"prose-em",h1:"prose-h1",h2:"prose-h2",h3:"prose-h3",h4:"prose-h4",h5:"prose-h5",h6:"prose-h6",hr:"prose-hr",img:"prose-img",ul:"prose-ul",ol:"prose-ol",li:"prose-li",strong:"prose-strong",table:"prose-table",thead:"prose-thead",tbody:"prose-tbody",td:"prose-td",th:"prose-th",tr:"prose-tr"},highlight:{theme:"dracula",preload:[A]},wsUrl:n,documentDriven:{page:u,navigation:u,surround:u,globals:{},layoutFallbacks:["theme"],injectPage:u},host:n,trailingSlash:p,anchorLinks:{depth:4,exclude:[1]}}},app:{baseURL:ag,buildAssetsDir:"\u002F_nuxt\u002F",cdnURL:n}},prerenderedAt:void 0}}("element","span","text","ct-fb4c3f","ct-aee26f","line"," ","ct-11e560","{","ct-c3f334","}",",","=","","    ",false,"p","  ","default","strong",true,"Deep Spectrum Feature Representations for Speech Emotion Recognition","code",2,"h2","a","bibtex","project","nofollow","img",null,"pre","author","title","year","Yiqin Zhao","markdown","content","md","system","\u002Fproject\u002Fdeep-spectrum","research","deep-spectrum-feature-representations-for-speech-emotion-recognition","ASMMC-MMAC'18","2018","pages","7","publisher","hr","exploring-deep-spectrum-representations-via-attention-based-recurrent-and-convolutional-neural-networks-for-speech-emotion-recognition","Exploring Deep Spectrum Representations via Attention-Based Recurrent and Convolutional Neural Networks for Speech Emotion Recognition","@article","exploring-spatio-temporal-representations-by-integrating-attention-based-bidirectional-lstm-rnns-and-fcns-for-speech-emotion-recognition","Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition","contact","Contact","\u002Fproject\u002F2048",2048,"\u002Fproject\u002Flitar","LitAR: Visually Coherent Lighting for Mobile Augmented Reality","\u002F")))}</script><script type="module" src="/_nuxt/entry.d85a4529.js" crossorigin></script><script type="module" src="/_nuxt/document-driven.6e49f9c7.js" crossorigin></script><script type="module" src="/_nuxt/default.8d670c31.js" crossorigin></script><script type="module" src="/_nuxt/Navigator.b794c455.js" crossorigin></script><script type="module" src="/_nuxt/ContentDoc.a13db5df.js" crossorigin></script><script type="module" src="/_nuxt/ContentQuery.39a448cb.js" crossorigin></script><script type="module" src="/_nuxt/Footer.0d7cbc0a.js" crossorigin></script><script type="module" src="/_nuxt/ContentRenderer.8604f978.js" crossorigin></script><script type="module" src="/_nuxt/ContentRendererMarkdown.aeb4793c.js" crossorigin></script><script type="module" src="/_nuxt/MarkdownHeader.7828b21e.js" crossorigin></script><script type="module" src="/_nuxt/ProseH2.ef74261f.js" crossorigin></script><script type="module" src="/_nuxt/ProseP.0b5d795b.js" crossorigin></script><script type="module" src="/_nuxt/ProseStrong.e42423de.js" crossorigin></script><script type="module" src="/_nuxt/ProseA.715696e3.js" crossorigin></script><script type="module" src="/_nuxt/ProseImg.b8b87e5a.js" crossorigin></script><script type="module" src="/_nuxt/ProseCode.c37abd21.js" crossorigin></script><script type="module" src="/_nuxt/ProseHr.10c0999c.js" crossorigin></script></body>
</html>