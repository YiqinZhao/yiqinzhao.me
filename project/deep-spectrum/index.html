<html><style>body{margin:0}img{width:100%}a{color:var(--font-color)}</style><head><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta charset="utf-8"><link rel="stylesheet" href="/assets/css/base.css"><link rel="stylesheet" href="/assets/css/prism-theme.css"><link rel="stylesheet" href="/assets/css/dark-mode.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css"><title>Deep Spectrum | Yiqin Zhao</title><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="apple-mobile-web-app-title" content="YiqinZhao"><link rel="apple-touch-icon" sizes="180x180" href="/assets/site-icons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/site-icons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/site-icons/favicon-16x16.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="shortcut icon" href="/assets/site-icons/favicon.ico" type="image/x-icon"><meta name="msapplication-TileColor" content="#ffffff"><meta name="theme-color" content="#ffffff"><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-4SWJ9K37XN"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4SWJ9K37XN")</script></head><style>.title{font-size:3rem;font-weight:700;margin:0 0 10px 0}.meta-row{border-bottom:1px dashed var(--line-color);margin-bottom:20px;opacity:.7;font-style:italic;flex-wrap:wrap;padding:10px 0}.meta-row>*{line-height:1.5rem}.date{margin-right:.5rem}.tag{margin-right:.5rem}.hero-wrapper{background-color:#f4f5f7;padding:0 0 40px 0}.hero-container{padding:20px 0;position:relative;align-items:flex-start}.markdown strong>img{border:1px solid rgba(0,0,0,.6)}@media (max-width:480px){.meta-row{flex-wrap:wrap}}@media (prefers-color-scheme:dark){.label-icon{filter:invert(1)}.markdown strong>img{filter:invert(.85)}}</style><body><header><style>.nav-container{width:100%;height:60px;padding:20px 0;display:flex;flex-direction:row;align-items:center;justify-content:space-between}.logo{font-size:1.5rem;font-weight:700;opacity:.7;cursor:pointer;color:var(--font-color);transition:opacity .1s linear}.logo:hover{opacity:1}.menu-item{font-size:1.3rem;font-weight:700;opacity:.7;text-transform:capitalize;margin:0 17px;padding:0 5px;cursor:pointer;transition:opacity .1s linear,border-bottom .1s linear}.menu-item:hover{opacity:1;border-bottom:3px solid #979797}.menu-item-active{opacity:1;border-bottom:3px solid #979797}.menu-item:nth-last-child(1){margin-right:0}.menu-row>a{color:var(--font-color)}.menu-row>a:hover{text-decoration:none}.menu-button{display:none}.mobile-nav{width:100%;display:none}@media (prefers-color-scheme:dark){#menu-button-icon{filter:invert()}.nav-container{background-color:transparent}}@media (min-width:320px) and (max-width:768px){body{padding-top:61px}.hero-header{overflow:hidden}.logo{display:none}.nav-container{padding:0 0;background-color:rgba(255,255,255,.5);position:fixed;top:0;border-bottom:1px solid #636363;backdrop-filter:blur(10px);-webkit-backdrop-filter:blur(10px);z-index:1;justify-content:flex-end}.menu-item-active{border-bottom:none}.menu-button{display:block;margin-right:20px}#menu-button-icon{height:17px;width:24px}.mobile-nav{height:60px;display:flex}.mobile-nav>.nav-title{font-size:1.3rem;font-weight:700;text-transform:capitalize;margin:0 17px;padding:0 5px}.nav-container{height:auto;flex-direction:column}.menu-row{padding:5px 0;flex-wrap:wrap;overflow:hidden;justify-content:space-between}.hide-menu-row{height:0;padding:0}.menu-row>.menu-item{margin:10px 17px}}@media (prefers-color-scheme:dark) and (min-width:320px) and (max-width:768px){.nav-container{background-color:rgba(0,0,0,.5)}}</style><div class="nav-container"><a class="logo" href="/" style="visibility:visible">Yiqin Zhao</a><div class="mobile-nav stack" horizontal-layout="space-between" vertical-layout="center"><div class="stack nav-title">project</div><div class="menu-button" onclick="onMenuButtonClick()"><img id="menu-button-icon" src="/assets/img/menu.svg" alt=""></div></div><div class="stack menu-row hide-menu-row"><a class="menu-item" href="/">home</a> <a class="menu-item" href="/news/">news</a> <a class="menu-item menu-item-active" href="/project/">project</a> <a class="menu-item" href="/publication/">publication</a> <a class="menu-item" href="/blog/">blog</a> <a class="menu-item" href="/assets/yiqinzhao-cv.pdf">CV</a></div></div><script>function onMenuButtonClick(){let e=document.querySelector(".menu-row").classList;e.contains("hide-menu-row")?(e.remove("hide-menu-row"),document.querySelector("#menu-button-icon").setAttribute("src","/assets/img/close.svg")):(e.add("hide-menu-row"),document.querySelector("#menu-button-icon").setAttribute("src","/assets/img/menu.svg"))}</script></header><article class="stack" direction="column"><div class="stack" direction="column"><div class="title">Deep Spectrum</div><div class="stack meta-row" vertical-layout="center"><div class="date">Nov 19, 2019</div><span class="tag stack" vertical-layout="center"><img class="vector-image" src="/assets/icons/tag.svg" alt=""> Research</span><div class="eta">ETA: 3min(s)</div></div></div><div class="markdown"><h1 id="paper">Paper</h1><p>Zhao, Ziping and <strong>Zhao, Yiqin</strong> and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao</p><p><strong>Deep Spectrum Feature Representations for Speech Emotion Recognition</strong></p><p>Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data (<strong>ASMMC-MMAC'18</strong>)</p><p><a href="https://dl.acm.org/doi/10.1145/3267935.3267948"><strong><img src="thumbnail-asmmc-18.png" alt=""></strong></a></p><pre class="language-bibtex"><code class="language-bibtex"><span class="token type function bold">@inproceedings</span><span class="token punctuation">{</span><span class="token name regex">deep_spectrum_2018</span><span class="token punctuation">,</span>
    <span class="token field keyword">author</span> <span class="token punctuation">=</span> <span class="token value char">{Zhao, Ziping and Zhao, Yiqin and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao}</span><span class="token punctuation">,</span>
    <span class="token field keyword">title</span> <span class="token punctuation">=</span> <span class="token value char">{Deep Spectrum Feature Representations for Speech Emotion Recognition}</span><span class="token punctuation">,</span>
    <span class="token field keyword">booktitle</span> <span class="token punctuation">=</span> <span class="token value char">{Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia
    Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data}</span><span class="token punctuation">,</span>
    <span class="token field keyword">series</span> <span class="token punctuation">=</span> <span class="token value char">{ASMMC-MMAC'18}</span><span class="token punctuation">,</span>
    <span class="token field keyword">year</span> <span class="token punctuation">=</span> <span class="token value char">{2018}</span><span class="token punctuation">,</span>
    <span class="token field keyword">isbn</span> <span class="token punctuation">=</span> <span class="token value char">{978-1-4503-5985-6}</span><span class="token punctuation">,</span>
    <span class="token field keyword">location</span> <span class="token punctuation">=</span> <span class="token value char">{Seoul, Republic of Korea}</span><span class="token punctuation">,</span>
    <span class="token field keyword">pages</span> <span class="token punctuation">=</span> <span class="token value char">{27--33}</span><span class="token punctuation">,</span>
    <span class="token field keyword">numpages</span> <span class="token punctuation">=</span> <span class="token value char">{7}</span><span class="token punctuation">,</span>
    <span class="token field keyword">url</span> <span class="token punctuation">=</span> <span class="token value char">{http://doi.acm.org/10.1145/3267935.3267948}</span><span class="token punctuation">,</span>
    <span class="token field keyword">doi</span> <span class="token punctuation">=</span> <span class="token value char">{10.1145/3267935.3267948}</span><span class="token punctuation">,</span>
    <span class="token field keyword">acmid</span> <span class="token punctuation">=</span> <span class="token value char">{3267948}</span><span class="token punctuation">,</span>
    <span class="token field keyword">publisher</span> <span class="token punctuation">=</span> <span class="token value char">{ACM}</span><span class="token punctuation">,</span>
    <span class="token field keyword">address</span> <span class="token punctuation">=</span> <span class="token value char">{New York, NY, USA}</span><span class="token punctuation">,</span>
    <span class="token field keyword">keywords</span> <span class="token punctuation">=</span> <span class="token value char">{attention mechanism, bidirectional long short-term memory,
    fully convolutional networks, spectrogram representation, speech emotion recognition}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><hr><p>Ziping Zhao, Zhongtian Bao, <strong>Yiqin Zhao</strong>, Zixing Zhang, Nicholas Cummins, Zhao Ren, Björn Schuller</p><p><strong>Exploring Deep Spectrum Representations via Attention-Based Recurrent and Convolutional Neural Networks for Speech Emotion Recognition</strong></p><p><strong>IEEE Access 2019</strong></p><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762126"><strong><img src="thumbnail-ieee-19.png" alt=""></strong></a></p><pre class="language-bibtex"><code class="language-bibtex"><span class="token type function bold">@article</span><span class="token punctuation">{</span><span class="token name regex">zhao2019exploring</span><span class="token punctuation">,</span>
  <span class="token field keyword">title</span><span class="token punctuation">=</span><span class="token value char">{Exploring deep spectrum representations via attention-based recurrent and
  convolutional neural networks for speech emotion recognition}</span><span class="token punctuation">,</span>
  <span class="token field keyword">author</span><span class="token punctuation">=</span><span class="token value char">{
    Zhao, Ziping and Bao, Zhongtian and Zhao, Yiqin and Zhang, Zixing and Cummins,
    Nicholas and Ren, Zhao and Schuller, Bj{<span class="token tag">\"</span>o}rn}</span><span class="token punctuation">,</span>
  <span class="token field keyword">journal</span><span class="token punctuation">=</span><span class="token value char">{IEEE Access}</span><span class="token punctuation">,</span>
  <span class="token field keyword">volume</span><span class="token punctuation">=</span><span class="token value char">{7}</span><span class="token punctuation">,</span>
  <span class="token field keyword">pages</span><span class="token punctuation">=</span><span class="token value char">{97515--97525}</span><span class="token punctuation">,</span>
  <span class="token field keyword">year</span><span class="token punctuation">=</span><span class="token value char">{2019}</span><span class="token punctuation">,</span>
  <span class="token field keyword">publisher</span><span class="token punctuation">=</span><span class="token value char">{IEEE}</span>
<span class="token punctuation">}</span>
</code></pre><hr><p>Ziping Zhao, Yu Zheng, Zixing Zhang, Haishuai Wang, <strong>Yiqin Zhao</strong>, Chao Li.</p><p><strong>Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition</strong></p><p>Annual Conference of the International Speech Communication Association, <strong>INTERSPEECH 2018</strong></p><p><a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1477.pdf"><strong><img src="thumbnail-interspeech-18.png" alt=""></strong></a></p><pre class="language-bibtex"><code class="language-bibtex"><span class="token type function bold">@article</span><span class="token punctuation">{</span><span class="token name regex">zhao2018exploring</span><span class="token punctuation">,</span>
  <span class="token field keyword">title</span><span class="token punctuation">=</span><span class="token value char">{Exploring spatio-temporal representations by integrating
  attention-based bidirectional-LSTM-RNNs and FCNs for speech emotion recognition}</span><span class="token punctuation">,</span>
  <span class="token field keyword">author</span><span class="token punctuation">=</span><span class="token value char">{Zhao, Ziping and Zheng, Yu and Zhang, Zixing and Wang, Haishuai and Zhao, Yiqin and Li, Chao}</span><span class="token punctuation">,</span>
  <span class="token field keyword">year</span><span class="token punctuation">=</span><span class="token value char">{2018}</span>
<span class="token punctuation">}</span>
</code></pre><h1 id="contact">Contact</h1><p>If you have any questions, please feel free to contact <a href="mailto:yiqinzhao@outlook.com">Yiqin Zhao</a>.</p></div></article><style>.foot{opacity:.6;border-top:1px solid var(--line-color);margin-top:20px;padding:20px 0;font-size:.8rem}.foot a{color:var(--font-color);text-decoration:underline}</style><footer class="stack foot" direction="column" horizontal-layout="center"><span>© <a href="/">Yiqin Zhao</a> 2021. Last Updated: May 2, 2021</span> <span>Proudly Powered by <a href="https://github.com/YiqinZhao/Ploceus">Ploceus</a></span></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/sweetalert/2.1.2/sweetalert.min.js" integrity="sha512-AA1Bzp5Q0K1KanKKmvN/4d3IRKVlv9PYgwFPvm32nPO6QS8yH1HO7LbgB1pgiOxPtfeg5zEn2ba64MUcqJx6CA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/crypto-js/4.0.0/crypto-js.min.js" integrity="sha512-nOQuvD9nKirvxDdvQ9OMqe2dgapbPB7vYAMrzJihw5m+aNcf0dX53m6YxM4LgA9u8e9eg9QX+/+mPu8kCNpV2A==" crossorigin="anonymous"></script><script>window.addEventListener("load",function(){document.querySelectorAll(".encrypted").forEach(e=>{e.addEventListener("click",function(){this.classList.contains("encrypted")&&swal("Password",{content:"input"}).then(t=>{let e=this;for(;!e.classList.contains("markdown");)e=e.parentNode;e.querySelectorAll(".encrypted").forEach(e=>{e.innerHTML=CryptoJS.AES.decrypt(e.innerHTML,t).toString(CryptoJS.enc.Utf8),e.classList.remove("encrypted"),e.classList.add("decrypted")})})})})})</script></body></html>