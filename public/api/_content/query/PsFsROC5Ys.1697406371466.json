{"_path":"/research","_dir":"","_draft":false,"_partial":false,"_locale":"","title":"Research","description":"","body":{"type":"root","children":[{"type":"element","tag":"MarkdownHeader","props":{"title":"Research Statement"},"children":[]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"I'm a computer science Ph.D. candidate at "},{"type":"element","tag":"a","props":{"href":"https://wpi.edu","rel":["nofollow"]},"children":[{"type":"text","value":"Worcester Polytechnic Institute"}]},{"type":"text","value":" and a member of "},{"type":"element","tag":"a","props":{"href":"https://cake.wpi.edu","rel":["nofollow"]},"children":[{"type":"text","value":"TheCakeLab"}]},{"type":"text","value":".\nMy research interest lies in ubiquitous computing, augmented reality, and computer vision.\nMy recent research projects have a strong focus on improving photorealism in mobile AR by developing novel environment lighting sensing systems.\nThe insights of my research are mainly derived from the investigation of mobile device dynamics, physical modeling of spatial transformations, and data-driven studies.\nIn the past, I have designed machine learning models and real-time systems for mobile AR environment lighting estimation.\nI’m also interested in security and privacy issues associated with photorealism in AR.\nI’m fortunate to have conducted my research works in these areas under the guidance of Prof. Tian Guo alongside many wonderful collaborators, and I’m passionate to continue studying these problems in my Ph.D."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Designing visual sensing systems has been a long-standing research topic for ubiquitous computing, mobile computing and computer vision.\nIn recent years, emerging AR applications raise new standards for environment sensing on mobile devices to satisfy the crave of increasing demand of photorealism and interactivity.\nLighting estimation, a task that aims to estimate omnidirectional lighting from limited environment observations, is at the key position of pursuing photorealistic and visually coherent rendering in AR.\nTraditionally, high-quality environment lighting is acquired using physical devices like light probes or 360° cameras.\nSuch physical approach has satisfied the needs of professional many users from filming industries and computer graphics researchers.\nHowever, the data acquisition process could be expensive and impractical for mobile AR applications."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"On a different vein, learning-based lighting estimation models have been adopted by the majority of AR frameworks in favor of their abilities to estimate environment lighting from single or multiple camera images.\nHowever, in complex real-world scenarios, such models often fail to adapt, as estimating 360° environment lighting from a low FoV camera image is a highly unconstrained task.\nThe deployment time difficulties challenges the traditional definition of lighting estimation, which aims to use image color and structure clues to estimate the environment lighting.\nMy past research has focused on leveraging user, device and environment dynamics to opportunistically generate lighting without interrupting user's application interaction."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"text","value":"Check out my research project details "},{"type":"element","tag":"a","props":{"href":"/project/"},"children":[{"type":"text","value":"here"}]},{"type":"text","value":"."}]},{"type":"element","tag":"p","props":{},"children":[{"type":"element","tag":"em","props":{},"children":[{"type":"text","value":"I'm generally interested in collaborating with ubiquitous computing and machine learning projects related to AR/VR. If you share the same interests, "},{"type":"element","tag":"a","props":{"href":"mailto:yzhao11@wpi.edu"},"children":[{"type":"text","value":"please shoot me an email"}]},{"type":"text","value":"!"}]}]}],"toc":{"title":"","searchDepth":2,"depth":2,"links":[]}},"_type":"markdown","_id":"content:research.md","_source":"content","_file":"research.md","_extension":"md"}